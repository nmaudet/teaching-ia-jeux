{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamiques multiagents\n",
    "\n",
    "\n",
    "> **Rappel**: si la visualisation (en particulier des commandes LaTeX) est défaillante, ouvrir le même fichier dans le [viewer jupyter](https://nbviewer.jupyter.org/github/nmaudet/teaching-iaro/blob/master/3-dynamiquesMultiagents/DynamiquesMultiagents.ipynb)\n",
    "\n",
    "## Caractérisation des stratégies des agents\n",
    "\n",
    "Reprenons les stratégies envisagées dans le cadre du dilemme du prisonnier. \n",
    "On voit que l'on peut les classer selon deux critères: \n",
    "\n",
    "* le **niveau d'information** requis par la règle: on parle d'une règle de niveau $k$ lorsque les $k$ derniers coups de l'adversaire sont requis. \n",
    "* le fait que la règle soit **déterministe ou stochastique**\n",
    "\n",
    "On peut parler d'apprentissage dès qu'une règle utilise la connaissance des coups joués par les autres joueurs, autrement dit dès qu'elle est de niveau $k\\geq 1$. \n",
    "\n",
    "> Pouvez-vous caractériser selon ces critères les règles random, tit-for-tat, toujours trahir...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Meilleures réponses avec plusieurs agents\n",
    "\n",
    "On a vu dans les cours précédents la notion de meilleure réponse. \n",
    "Remarquons tout d'abord que cette notion s'étend tout à fait naturellement à plus de deux agents. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque le système multiagent comporte plus de deux agents, on peut construire de manière similaire les tables de meilleures réponses, mais selon les possibilités de stratégies de **tous** les autres agents. Par exemple, pour décrire les meilleures réponses d'un agent x aux actions de $x_1, x_2, x_3$, on aura une table de ce type: \n",
    "\n",
    "| BR(x)  | $x_1$  | $x_2$ | $x_3$ |\n",
    "|------|------|------|------|\n",
    "| acheter  | emprunter| emprunter  | emprunter |\n",
    "| emprunter  | acheter| acheter  | emprunter |\n",
    "| ... | ... | ... | ... |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Problème:** la table de meilleure réponse grandit exponentiellement avec le nombre d'agents. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toutefois, dans de nombreuses situations, les actions des agents n'ont que des conséquences locales. On peut donc limiter la représentation des meilleures réponses aux agents concernés (par exemple aux voisins sur un graphe). On parle alors de **jeux graphiques**. \n",
    "\n",
    "**Exemple.** Imaginons par exemple que Riri, Fifi, et Loulou habitent dans une rue: Riri au début de la rue, Fifi au milieu de la rue, et Loulou à la fin de la rue. Les trois amis se demandent s'ils doivent acheter une tondeuse à gazon ou plutôt l'emprunter à leur voisin. Cette décision dépend du choix de leur voisin. Riri est voisin de Fifi, Fifi est voisin de Riri et Loulou, et Loulou n'est voisin que de Riri. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bestResponse(s,agent):\n",
    "    if agent == 'riri' or agent=='loulou':\n",
    "        if s['fifi']=='achete':\n",
    "            return 'emprunte'\n",
    "        else:\n",
    "            return 'achete'\n",
    "    elif agent == 'fifi':\n",
    "        if s['riri']=='emprunte' and s['loulou']=='emprunte':\n",
    "            return 'achete'\n",
    "        else: \n",
    "            return 'emprunte'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dynamiques de meilleures réponses\n",
    "On peut sur cette base étudier le processus dynamique qui résulte du fait que les agents jouent itérativement des meilleures réponses. \n",
    "\n",
    "De manière générale, on peut parler **partial best-response**: une partie de la population d'agents joue une meilleure réponse à l'état courant. On a donc deux cas limites: \n",
    "\n",
    "* la situation où **un seul agent** est considéré à chaque fois: les actions sont donc modifiée \n",
    "* la situation où **tous les agents** modifient leurs stratégies simultanément à chaque fois, en réponse à l'observation de l'état courant. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la version \"agent unique\", l'algorithme de dynamique de meilleure réponse prend donc la simple forme suivante: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loulou': 'achete', 'riri': 'achete', 'fifi': 'achete'}\n",
      "{'loulou': 'emprunte', 'riri': 'achete', 'fifi': 'achete'}\n",
      "{'loulou': 'emprunte', 'riri': 'emprunte', 'fifi': 'achete'}\n",
      "{'loulou': 'emprunte', 'riri': 'emprunte', 'fifi': 'achete'}\n",
      "{'loulou': 'emprunte', 'riri': 'emprunte', 'fifi': 'achete'}\n",
      "{'loulou': 'emprunte', 'riri': 'emprunte', 'fifi': 'achete'}\n",
      "{'loulou': 'emprunte', 'riri': 'emprunte', 'fifi': 'achete'}\n"
     ]
    }
   ],
   "source": [
    "agents = ['loulou', 'riri', 'fifi']\n",
    "s= {'riri': 'achete', 'fifi': 'achete', 'loulou': 'achete'} # vecteur de stratégies initiales\n",
    "equilibrium = False\n",
    "print (s)\n",
    "while not equilibrium:\n",
    "    equilibrium = True\n",
    "    for i in agents:\n",
    "        if s[i]!=bestResponse(s,i):\n",
    "            s[i]=bestResponse(s,i)\n",
    "            equilibrium=False\n",
    "        print (s)\n",
    "    \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Quelle est l'autre équilibre de Nash de ce jeu? Pouvez-vous trouver une séquence des agents menant (depuis le même état initial) à cet équilibre? \n",
    "> Que se passe-t-il sur cet exemple si on considère la dynamique où tous les agents modifient simultanément leurs stratégies?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quelques propriétés simples des dynamiques de meilleures réponses: \n",
    "\n",
    "* Lorsqu'un équilibre est atteint, il s'agit bien d'un équilibre de Nash (par définition). \n",
    "* Les dynamiques de meilleures réponses cyclent nécessairement lorsqu'il n'y a pas d'équilibre de Nash\n",
    "* Les dynamiques de meilleures réponses peuvent cycler *même si il existe des équilibres de Nash* \n",
    "* Toutefois, sous certaines conditions, on peut montrer qu'elles convergent nécessairement vers des équilibres de Nash. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dynamiques de réponses améliorantes\n",
    "\n",
    "Au lieu de supposer que les agents changent de stratégie en choisissant la meilleure (ou une des meilleures s'il en existe plusieurs) réponse améliorante, on peut supposer que les réponses sont seulement améliorantes (donc meilleures que la stratégie actuelle, sans être nécessairement *la* meilleure)\n",
    "\n",
    "### Dynamiques de mariages stables\n",
    "\n",
    "Reprenons le cadre des mariages que vous avez étudié en détail avec Bruno Escoffier, et essayons de définir ce que pourrait être une dynamique de meilleure réponse, ou de réponse améliorante. \n",
    "\n",
    "On rappelle qu'une paire instable est une paire d'agents qui préférent être ensemble qu'avec leur partenaire actuel. \n",
    "Dans notre cadre, on dit qu'on résoud satisfait une paire lorsque l'on associe les deux agents de la paire. Les anciens partenaires se trouvent alors non associés. \n",
    "\n",
    "Prenons le cadre suivant: \n",
    "\n",
    "* on part d'une situation initiale, c'est-à-dire un matching (éventuellement partiel) entre les agents de M et W. \n",
    "* on considère une paire d'agents instable $(m,w) \\in M \\times W$. \n",
    "* dans une dynamique de réponse améliorante, on satisfait cette paire instable.  \n",
    "* dans une dynamique de meilleure réponse, on considère un agent impliqué dans cette paire instable (par exemple $m$), et on satisfait sa paire instable préférée parmi toutes les paires instables dans lesquelles il est impliqué.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Knuth (1976) a montré que la dynamique de réponse améliorante ne converge pas forcément. \n",
    "Son exemple original était le suivant: \n",
    "\n",
    "| women | men |\n",
    "| ----- | ----- |\n",
    "|  w1: m1, m3, m2 |  m1: w2, w1, w3 |\n",
    "|  w2: m3, m1, m2 | m2: w1, w3, w2 |\n",
    "|  w3: m1, m3, m2 | m3: w1, w2, w3 |\n",
    "\n",
    "Vous avez vu en TD d'autres exemples de tels cycles (procédure des divorces successifs). \n",
    "\n",
    "Le même type de cyle peut être obtenu pour la dynamique de meilleure réponse (Ackerman et al., 2008). \n",
    "\n",
    "Voici l'exemple: \n",
    "\n",
    "| women | men |\n",
    "| ----- | ----- |\n",
    "|  w1: m2, m3, m1 | m1: w1, w3, w2 |\n",
    "|  w2: m1, m2, m3 | m2: w2, w1, w3 |\n",
    "|  w3: m3, m1, m2 | m3: w1, w2, w3 |\n",
    "\n",
    "Un cycle est le suivant (le . représente le fait que le femme d'indice $i$ n'est pas en couple): $(.,w2-m2,w3-m3) \\rightarrow (w1-m3,w2-m2,.) \\rightarrow (w1-m3,w2-m1,.) \\rightarrow (w1-m3,.,w3-m1) \\rightarrow  (w1-m2,.,w3-m1)  \\rightarrow(.,w2-m2,w3-m1)  \\rightarrow (.,w2-m2,w3-m3)$\n",
    "\n",
    "\n",
    "De là découlent de nombreuses autres questions naturelles: \n",
    "\n",
    "* peut-on toujours une séquence qui mène à un état stable? \n",
    "* si la dynamique est aléatoire, quelle est la probabilité d'atteindre un état stable? \n",
    "* combien de temps peut-il falloir pour atteindre un état stable? \n",
    "* si on part d'un état initial particulier (par exemple le matching vide), est-ce que cela peut changer les choses? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fictitious Play\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposons maintenant que les agents apprennent plus spécifiquement le comportement des autres joueurs, et jouent la meilleure réponse à la prédiction qu'ils font du comportement des autres agents. \n",
    "Un des modèles les plus simples d'un tel type d'apprentissage a été proposé par Brown (1951) sous le nom de *fictitious play*. \n",
    "\n",
    "Il repose donc sur les phases suivantes: \n",
    "\n",
    "* prédiction des actions des autres joueurs\n",
    "* choix de la meilleure réponse à la stratégie des autres agents\n",
    "* observation des actions des autres joueurs, mise à jour de la prédiction\n",
    "\n",
    "**(Standard) Fictitious Play**: \n",
    "Nous présentons ici le modèle dans un contexte où seuls deux agents sont en interaction à chaque fois, même si le système comprend plus d'agents. (Le modèle s'étend à des interactions impliquant plus d'agents, mais il faut alors faire des choix sur la manière de combiner les probabilités assignées aux différents agents). \n",
    "\n",
    "* Il faut d'abord que l'agent fasse une prédiction *a priori* sur l'action initiale de l'agent (cette prédiction peut être une distribution de proba, pas nécessairement un coup donné). \n",
    "* Chaque joueur tient à jour la fréquence de sélection des actions et joue sa meilleure réponse à ce modèle probabiliste. \n",
    "\n",
    "Ici la meilleure réponse est entendu en terme de gain espéré pour chacune de ses actions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Exemple: \n",
    "\n",
    "Plaçons nous du point de vue l'agent <span style=\"color:red\">R</span>, qui est opposé à l'agent <span style=\"color:blue\">B</span>. \n",
    "La table suivante donne les gains pour le joueur R. \n",
    "\n",
    "|<span style=\"color:red\">R</span> \\ <span style=\"color:blue\">B</span> | c | d |\n",
    "| ----|---- | ---- |\n",
    "|**a** | 7 | 10 |\n",
    "|**b** | 10 | 0 |\n",
    "\n",
    "\n",
    "Supposons que la stratégie de B soit de jouer c et d à 50/50. \n",
    "Et que R ait comme a priori que B va presque certainement toujours jouer c ($P(c)=0.8$), qui provient d'un échantillon initial de 4 c et 1 d. \n",
    "\n",
    "\n",
    "\n",
    "|  #c | #d | P(c) | P(d) |  BR(R) | coup B |\n",
    "| ----|---- | ---- | ---- | ---- | ---- |\n",
    "|  4 | 1 | 0.8 | 0.2 |  b | c |\n",
    "|  5 | 1 | 0.83 | 0.17 |  b | d | \n",
    "|  5 | 2 | 0.71 | 0.29 |  a | c | \n",
    "\n",
    "Pourquoi a est-elle la meilleure réponse à l'itération 3? \n",
    "\n",
    "On voit qu'à cette étape, R estime que B joue c et d avec une probabilité 50/50. \n",
    "Quelle est le gain espéré pour lui de jouer \n",
    "* l'action a: $0.71 \\times 7 + 0.29 \\times 10 = 7.86$\n",
    "* l'action b: $0.71 \\times 10 + 0.29 \\times 0 = 7.1$\n",
    "\n",
    "On a donc gain espéré pour a > gain espéré pour b. La meilleure réponse est de jouer a. \n",
    "\n",
    "Et ainsi de suite pour les itérations suivantes...\n",
    "\n",
    "|  #c | #d | P(c) | P(d) |  BR(R) | coup B |\n",
    "| ----|---- | ---- | ---- | ---- | ---- |\n",
    "|  6 | 2 | 0.75 | 0.25 |  a | d |\n",
    "|  6 | 3 | 0.66 | 0.33 |  a | c |\n",
    "| ... | ... | ... | ... | ... | ... |\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Mais notons que l'apprentissage reste rudimentaire. Par exemple, si j'observe que l'autre joueur joue *alternativement* c et d, un agent utilisant fictitious play attribuera à la limite une probabilité 50/50 pour l'autre agent de jouer c ou d, alors que s'il avait reconnu la séquence, il serait capable d'obtenir un bien meilleur gain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'b': 1, 'a': 2}, {'b': 2, 'a': 1}]\n",
      "joueur  0\n",
      "gain espéré pour  a 2.0\n",
      "gain espéré pour  b 0.6666666666666666\n",
      "joueur  1\n",
      "gain espéré pour  a 0.6666666666666666\n",
      "gain espéré pour  b 2.0\n",
      "b b\n",
      "[{'b': 2, 'a': 2}, {'b': 3, 'a': 1}]\n",
      "joueur  0\n",
      "gain espéré pour  a 2.0\n",
      "gain espéré pour  b 1.3333333333333333\n",
      "joueur  1\n",
      "gain espéré pour  a 0.6666666666666666\n",
      "gain espéré pour  b 3.0\n",
      "b b\n",
      "[{'b': 3, 'a': 2}, {'b': 4, 'a': 1}]\n",
      "joueur  0\n",
      "gain espéré pour  a 2.0\n",
      "gain espéré pour  b 2.0\n",
      "joueur  1\n",
      "gain espéré pour  a 0.6666666666666666\n",
      "gain espéré pour  b 4.0\n",
      "b b\n"
     ]
    }
   ],
   "source": [
    "actions = [('a','b'),('a','b')]\n",
    "matGains ={'a':{'a':(3,2),'b':(0,0)},'b':{'a':(0,0),'b':(2,3)}}\n",
    "prior = [{'a':2,'b':1},{'a':1,'b':2}]\n",
    "expectedGain = {'a':0,'b':0}\n",
    "nbJoueurs = 2\n",
    "nbObservations = sum (prior[0].values())\n",
    "nbIterations = 3\n",
    "\n",
    "def fictitious(observations):\n",
    "    actionsChosen =[]\n",
    "    for i in range(nbJoueurs):\n",
    "        expectedGain = {'a':0,'b':0}\n",
    "        print ('joueur ',i)\n",
    "        for actionMe in actions[0]:\n",
    "            bestAction = actions[0][0]\n",
    "            bestGain =0\n",
    "            for actionOther in actions[1]: \n",
    "                expectedGain[actionMe] += (observations[i][actionMe]/nbObservations)*matGains[actionMe][actionOther][i]\n",
    "            print (\"gain espéré pour \",actionMe, expectedGain[actionMe])\n",
    "            if expectedGain[actionMe]>bestGain:\n",
    "                bestAction=actionMe\n",
    "                bestGain=expectedGain[actionMe]\n",
    "        actionsChosen+= bestAction\n",
    "    return actionsChosen\n",
    "\n",
    "observations = prior.copy()\n",
    "for i in range(nbIterations):\n",
    "    print (observations)\n",
    "    nextAction0, nextAction1 = fictitious(observations)\n",
    "    print (nextAction0,nextAction1)\n",
    "    observations[0][nextAction1]+=1\n",
    "    observations[1][nextAction0]+=1\n",
    "\n",
    "\n",
    "#fictitious(prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quelques propriétés des dynamiques fictitious play\n",
    "\n",
    "Supposons à présent que les deux agents utilisent une dynamique de fictitious play. \n",
    "On voit que l'on peut atteindre deux types de situations d'équilibre: \n",
    "* des équilibres pour lesquels les probabilités sont concentrées sur une action unique (équilibre pur)\n",
    "* des équilibres pour lesquels les probabilités sont réparties sur plusieurs actions (équilibre mixte)\n",
    "\n",
    "On sait alors que: \n",
    "\n",
    "* tout équilibre pur atteint par fictitious play doit être un équilibre de Nash\n",
    "* la dynamique n'atteint pas nécessairement de situation d'équilibre (même mixte)\n",
    "* il existe plusieurs catégories de jeux pour lesquels la convergence est garantie, par exemple les jeux à somme nulle. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Autres techniques d'apprentissage\n",
    "\n",
    "Considérons le problème suivant: chaque week-end des agents souhaitent se rendre dans un bar pour écouter de la musique. Les agents (supposons qu'ils sont au nombre de 100) ont la préférence suivante: ils souhaitent aller au bar pour écouter la musique, mais en même temps qu'il y ait trop de monde, sinon ils ne peuvent plus écouter correctement de la musique. \n",
    "\n",
    "Plus précisément: \n",
    "\n",
    "* si moins de 60 % de la population se rend au bar, l'agent préfère aller au bar\n",
    "* si plus de 60% de la population se rend au bar, l'agent aurait préféré rester chez lui. \n",
    "\n",
    "Ls agents se rendent le premier week-end au bar, ils observent la situation, puis révise leur stratégie pour le deuxième week-end, etc.\n",
    "\n",
    "Il s'agit d'un problème connu sous le nom de *El-Farol problem*, un jeu de minorité. \n",
    "\n",
    "On s'interroge sur la règle de décision que peuvent utiliser les agens dans cette seituation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Que se passe-t-il si tous les agents utilisent la même méthode déterministe pour décider s'ils vont au bar ou pas?\n",
    "\n",
    "> Que se passe-t-il si tous les agents utilisent pile ou face pour décider s'ils vont au bar ou pas?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arthur propose de faire reposer la décision des agents sur un **ensemble de prédicteurs**. \n",
    "\n",
    "* prédicteurs cycliques: même nombre de personnes qu'il y a *k* semaines\n",
    "* 100 - le nombre de personnes de la semaine dernière\n",
    "* prédicteur fixe: 67 personnes\n",
    "* moyenne glissante sur les 4 dernières semaines\n",
    "* la tendance sur les 8 dernières semaines évaluée avec une régression linéaire et la méthode des moindre carrés\n",
    "\n",
    "Chaque prédicteur se voit ensuite associé un score, selon la qualité de sa prédiction (+1 si la prédiction trop-de-monde / pas-trop-de-monde concorde avec l'observation). \n",
    "\n",
    "Une bibliothèque de prédicteurs est créée, puis chaque agent reçoit un ensemble de prédicteurs qu'il aura à sa disposition. A chaque tour, les agents utiliseront leur prédicteur le plus performant jusqu'alors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
