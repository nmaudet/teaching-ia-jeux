{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamiques multiagents\n",
    "\n",
    "\n",
    "> **Rappel**: si la visualisation (en particulier des commandes LaTeX) est défaillante, ouvrir le même fichier dans le [viewer jupyter](https://nbviewer.jupyter.org/github/nmaudet/teaching-iaro/blob/master/3-dynamiquesMultiagents/DynamiquesMultiagents.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Meilleures réponses avec plusieurs agents\n",
    "\n",
    "On a vu dans les cours précédents la notion de meilleure réponse. \n",
    "Remarquons tout d'abord que cette notion s'étend tout à fait naturellement à plus de deux agents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque le système multiagent comporte plus de deux agents, on peut construire de manière similaire les tables de meilleures réponses, mais selon les possibilités de stratégies de **tous** les autres agents. Par exemple, pour décrire les meilleures réponses d'un agent x aux actions de $x_1, x_2, x_3$, on aura une table de ce type: \n",
    "\n",
    "| BR(x)  | $x_1$  | $x_2$ | $x_3$ |\n",
    "|------|------|------|------|\n",
    "| acheter  | emprunter| emprunter  | emprunter |\n",
    "| emprunter  | acheter| acheter  | emprunter |\n",
    "| ... | ... | ... | ... |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Problème:** la table de meilleure réponse grandit exponentiellement avec le nombre d'agents. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toutefois, dans de nombreuses situations, les actions des agents n'ont que des conséquences locales. On peut donc limiter la représentation des meilleures réponses aux agents concernés (par exemple aux voisins sur un graphe). On parle alors de **jeux graphiques**. \n",
    "\n",
    "**Exemple.** Imaginons par exemple que Riri, Fifi, et Loulou habitent dans une rue: Riri au début de la rue, Fifi au milieu de la rue, et Loulou à la fin de la rue. Les trois amis se demandent s'ils doivent acheter une tondeuse à gazon ou plutôt l'emprunter à leur voisin. Cette décision dépend du choix de leur voisin. Riri est voisin de Fifi, Fifi est voisin de Riri et Loulou, et Loulou n'est voisin que de Riri. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bestResponse(s,agent):\n",
    "    if agent == 'riri' or agent=='loulou':\n",
    "        if s['fifi']=='achete':\n",
    "            return 'emprunte'\n",
    "        else:\n",
    "            return 'achete'\n",
    "    elif agent == 'fifi':\n",
    "        if s['riri']=='emprunte' and s['loulou']=='emprunte':\n",
    "            return 'achete'\n",
    "        else: \n",
    "            return 'emprunte'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dynamiques de meilleures réponses\n",
    "On peut sur cette base étudier le processus dynamique qui résulte du fait que les agents jouent itérativement des meilleures réponses. \n",
    "\n",
    "De manière générale, on peut parler **partial best-response**: une partie de la population d'agents joue une meilleure réponse à l'état courant. On a donc deux cas limites: \n",
    "\n",
    "* la situation où **un seul agent** est considéré à chaque fois: les actions sont donc modifiée \n",
    "* la situation où **tous les agents** modifient leurs stratégies simultanément à chaque fois, en réponse à l'observation de l'état courant. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la version \"agent unique\", l'algorithme de dynamique de meilleure réponse prend donc la simple forme suivante: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loulou': 'achete', 'riri': 'achete', 'fifi': 'achete'}\n",
      "{'loulou': 'emprunte', 'riri': 'achete', 'fifi': 'achete'}\n",
      "{'loulou': 'emprunte', 'riri': 'emprunte', 'fifi': 'achete'}\n",
      "{'loulou': 'emprunte', 'riri': 'emprunte', 'fifi': 'achete'}\n",
      "{'loulou': 'emprunte', 'riri': 'emprunte', 'fifi': 'achete'}\n",
      "{'loulou': 'emprunte', 'riri': 'emprunte', 'fifi': 'achete'}\n",
      "{'loulou': 'emprunte', 'riri': 'emprunte', 'fifi': 'achete'}\n"
     ]
    }
   ],
   "source": [
    "agents = ['loulou', 'riri', 'fifi']\n",
    "s= {'riri': 'achete', 'fifi': 'achete', 'loulou': 'achete'} # vecteur de stratégies initiales\n",
    "equilibrium = False\n",
    "print (s)\n",
    "while not equilibrium:\n",
    "    equilibrium = True\n",
    "    for i in agents:\n",
    "        if s[i]!=bestResponse(s,i):\n",
    "            s[i]=bestResponse(s,i)\n",
    "            equilibrium=False\n",
    "        print (s)\n",
    "    \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Quelle est l'autre équilibre de Nash de ce jeu? Pouvez-vous trouver une séquence des agents menant (depuis le même état initial) à cet équilibre? \n",
    "> Que se passe-t-il sur cet exemple si on considère la dynamique où tous les agents modifient simultanément leurs stratégies?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quelques propriétés simples des dynamiques de meilleures réponses: \n",
    "\n",
    "* Lorsqu'un équilibre est atteint, il s'agit bien d'un équilibre de Nash (par définition). \n",
    "* Les dynamiques de meilleures réponses cyclent nécessairement lorsqu'il n'y a pas d'équilibre de Nash\n",
    "* Les dynamiques de meilleures réponses peuvent cycler *même si il existe des équilibres de Nash* \n",
    "* Toutefois, sous certaines conditions, on peut montrer qu'elles convergent nécessairement vers des équilibres de Nash. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dynamiques de réponses améliorantes\n",
    "\n",
    "Au lieu de supposer que les agents changent de stratégie en choisissant la meilleure (ou une des meilleures s'il en existe plusieurs) réponse améliorante, on peut supposer que les réponses sont seulement améliorantes (donc meilleures que la stratégie actuelle, sans être nécessairement *la* meilleure)\n",
    "\n",
    "### Dynamiques de mariages stables\n",
    "\n",
    "Reprenons le cadre des mariages que vous avez étudié en détail avec Bruno Escoffier dans la première partie de ce cours, et essayons de définir ce que pourrait être une dynamique de meilleure réponse, ou de réponse améliorante. \n",
    "\n",
    "On rappelle qu'une paire instable est une paire d'agents qui préfèreraient être ensemble qu'avec leur partenaire actuel. \n",
    "Dans notre cadre, on dit qu'on satisfait une paire lorsque l'on associe les deux agents de cette paire. Les anciens partenaires se trouvent alors non associés. \n",
    "\n",
    "Prenons le cadre suivant: \n",
    "\n",
    "* on part d'une situation initiale, c'est-à-dire un matching (éventuellement partiel) entre les agents de $M$ et $W$. \n",
    "* on considère une paire d'agents instable $(m,w) \\in M \\times W$. \n",
    "* dans une dynamique de réponse améliorante, on satisfait cette paire instable.  \n",
    "* dans une dynamique de meilleure réponse, on considère un agent impliqué dans cette paire instable (par exemple $m$), et on satisfait sa paire instable préférée parmi toutes les paires instables dans lesquelles il est impliqué.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Knuth (1976) a montré que la dynamique de réponse améliorante ne converge pas forcément. \n",
    "Son exemple original était le suivant: \n",
    "\n",
    "| women | men |\n",
    "| ----- | ----- |\n",
    "|  w1: m1, m3, m2 |  m1: w2, w1, w3 |\n",
    "|  w2: m3, m1, m2 | m2: w1, w3, w2 |\n",
    "|  w3: m1, m3, m2 | m3: w1, w2, w3 |\n",
    "\n",
    "Vous avez vu en TD d'autres exemples de tels cycles (procédure des divorces successifs). \n",
    "\n",
    "Le même type de cyle peut être obtenu pour la dynamique de meilleure réponse (Ackerman et al., 2008). \n",
    "\n",
    "Voici l'exemple: \n",
    "\n",
    "| women | men |\n",
    "| ----- | ----- |\n",
    "|  w1: m2, m3, m1 | m1: w1, w3, w2 |\n",
    "|  w2: m1, m2, m3 | m2: w2, w1, w3 |\n",
    "|  w3: m3, m1, m2 | m3: w1, w2, w3 |\n",
    "\n",
    "Un cycle est le suivant (le . représente le fait que le femme d'indice $i$ n'est pas en couple): $(.,w2-m2,w3-m3) \\rightarrow (w1-m3,w2-m2,.) \\rightarrow (w1-m3,w2-m1,.) \\rightarrow (w1-m3,.,w3-m1) \\rightarrow  (w1-m2,.,w3-m1)  \\rightarrow(.,w2-m2,w3-m1)  \\rightarrow (.,w2-m2,w3-m3)$\n",
    "\n",
    "\n",
    "De là découlent de nombreuses autres questions naturelles: \n",
    "\n",
    "* peut-on toujours une séquence qui mène à un état stable? \n",
    "* si la dynamique est aléatoire, quelle est la probabilité d'atteindre un état stable? \n",
    "* combien de temps peut-il falloir pour atteindre un état stable? \n",
    "* si on part d'un état initial particulier (par exemple le matching vide), est-ce que cela peut changer les choses? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dynamiques avec apprentissage\n",
    "\n",
    "> Cette partie s'appuie notamment sur le Chapitre 7 de Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations (Shoham and Leyton-Brown)\n",
    "\n",
    "### 4.1 Caractérisation des stratégies des agents\n",
    "\n",
    "Reprenons les stratégies envisagées dans le cadre du dilemme du prisonnier. \n",
    "Rappelons que ces stratégies peuvent se baser sur les histoires (les séquences) des coups joués. \n",
    "On voit que l'on peut les classer selon deux critères: \n",
    "\n",
    "* le **niveau d'information** requis par la règle: on parle d'une règle de niveau $k$ lorsque les $k$ derniers coups de l'adversaire sont requis. \n",
    "* le fait que la règle soit **déterministe** (prescrive une action unique) ou **stochastique** (donne une distribution de probabilité sur des actions possibles à jouer)\n",
    "\n",
    "On peut parler d'apprentissage dès qu'une règle utilise la connaissance des coups joués par les autres joueurs, autrement dit dès qu'elle est de niveau $k\\geq 1$. Lorsque $k=0$, la stratégie est stationaire. \n",
    "\n",
    "> Pouvez-vous caractériser selon ces critères les règles random, tit-for-tat, toujours trahir..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Fictitious Play\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposons maintenant que les agents apprennent plus spécifiquement le comportement des autres joueurs, et jouent la meilleure réponse à la prédiction qu'ils font du comportement des autres agents. \n",
    "Un des modèles les plus simples d'un tel type d'apprentissage a été proposé par Brown (1951) sous le nom de *fictitious play*. \n",
    "\n",
    "Il repose donc sur les phases suivantes: \n",
    "\n",
    "* prédiction des actions des autres joueurs\n",
    "* choix de la meilleure réponse à la stratégie des autres agents\n",
    "* observation des actions des autres joueurs, mise à jour de la prédiction\n",
    "\n",
    "**(Standard) Fictitious Play**: \n",
    "Nous présentons ici le modèle dans un contexte où seuls deux agents sont en interaction à chaque fois, même si le système comprend plus d'agents. (Le modèle s'étend à des interactions impliquant plus d'agents, mais il faut alors faire des choix sur la manière de combiner les probabilités assignées aux différents agents). \n",
    "\n",
    "* Il faut d'abord que l'agent fasse une prédiction *a priori* sur l'action initiale de l'agent (cette prédiction peut être une distribution de proba, pas nécessairement un coup donné). \n",
    "* Chaque joueur tient à jour la fréquence de sélection des actions et joue sa meilleure réponse à ce modèle probabiliste. \n",
    "\n",
    "Ici la meilleure réponse est entendu en terme de gain espéré pour chacune de ses actions. \n",
    "L'hypothèse est donc que l'autre agent joue une stratégie stationaire. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Exemple: \n",
    "\n",
    "Plaçons nous du point de vue l'agent <span style=\"color:red\">R</span>, qui est opposé à l'agent <span style=\"color:blue\">B</span>. \n",
    "La table suivante donne les gains pour le joueur R. \n",
    "\n",
    "|<span style=\"color:red\">R</span> \\ <span style=\"color:blue\">B</span> | c | d |\n",
    "| ----|---- | ---- |\n",
    "|**a** | 7 | 10 |\n",
    "|**b** | 10 | 0 |\n",
    "\n",
    "\n",
    "Supposons que la stratégie de B soit de jouer c et d à 50/50. \n",
    "Et que R ait comme a priori que B va presque certainement toujours jouer c ($P(c)=0.8$), qui provient d'un échantillon initial de 4 c et 1 d. \n",
    "\n",
    "\n",
    "\n",
    "|  #c | #d | P(c) | P(d) |  BR(R) | coup B |\n",
    "| ----|---- | ---- | ---- | ---- | ---- |\n",
    "|  4 | 1 | 0.8 | 0.2 |  b | c |\n",
    "|  5 | 1 | 0.83 | 0.17 |  b | d | \n",
    "|  5 | 2 | 0.71 | 0.29 |  a | c | \n",
    "\n",
    "Pourquoi a est-elle la meilleure réponse à l'itération 3? \n",
    "\n",
    "On voit qu'à cette étape, R estime que B joue c et d avec une probabilité 50/50. \n",
    "Quelle est le gain espéré pour lui de jouer \n",
    "* l'action a: $0.71 \\times 7 + 0.29 \\times 10 = 7.86$\n",
    "* l'action b: $0.71 \\times 10 + 0.29 \\times 0 = 7.1$\n",
    "\n",
    "On a donc gain espéré pour a > gain espéré pour b. La meilleure réponse est de jouer a. \n",
    "\n",
    "Et ainsi de suite pour les itérations suivantes...\n",
    "\n",
    "|  #c | #d | P(c) | P(d) |  BR(R) | coup B |\n",
    "| ----|---- | ---- | ---- | ---- | ---- |\n",
    "|  6 | 2 | 0.75 | 0.25 |  a | d |\n",
    "|  6 | 3 | 0.66 | 0.33 |  a | c |\n",
    "| ... | ... | ... | ... | ... | ... |\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Mais notons que l'apprentissage reste rudimentaire. Par exemple, si j'observe que l'autre joueur joue *alternativement* c et d, un agent utilisant fictitious play attribuera à la limite une probabilité 50/50 pour l'autre agent de jouer c ou d, alors que s'il avait reconnu la séquence, il serait capable d'obtenir un bien meilleur gain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'a': 3, 'b': 1}, {'a': 1, 'b': 2}]\n",
      "joueur  0\n",
      "gain espéré pour  a 2.25\n",
      "gain espéré pour  b 0.5\n",
      "joueur  1\n",
      "gain espéré pour  a 0.6666666666666666\n",
      "gain espéré pour  b 2.0\n",
      "a b\n",
      "[{'a': 3, 'b': 2}, {'a': 2, 'b': 2}]\n",
      "joueur  0\n",
      "gain espéré pour  a 2.25\n",
      "gain espéré pour  b 1.0\n",
      "joueur  1\n",
      "gain espéré pour  a 1.3333333333333333\n",
      "gain espéré pour  b 2.0\n",
      "a b\n",
      "[{'a': 3, 'b': 3}, {'a': 3, 'b': 2}]\n",
      "joueur  0\n",
      "gain espéré pour  a 2.25\n",
      "gain espéré pour  b 1.5\n",
      "joueur  1\n",
      "gain espéré pour  a 2.0\n",
      "gain espéré pour  b 2.0\n",
      "a a\n",
      "[{'a': 4, 'b': 3}, {'a': 4, 'b': 2}]\n",
      "joueur  0\n",
      "gain espéré pour  a 3.0\n",
      "gain espéré pour  b 1.5\n",
      "joueur  1\n",
      "gain espéré pour  a 2.6666666666666665\n",
      "gain espéré pour  b 2.0\n",
      "a a\n",
      "[{'a': 5, 'b': 3}, {'a': 5, 'b': 2}]\n",
      "joueur  0\n",
      "gain espéré pour  a 3.75\n",
      "gain espéré pour  b 1.5\n",
      "joueur  1\n",
      "gain espéré pour  a 3.3333333333333335\n",
      "gain espéré pour  b 2.0\n",
      "a a\n",
      "[{'a': 6, 'b': 3}, {'a': 6, 'b': 2}]\n",
      "joueur  0\n",
      "gain espéré pour  a 4.5\n",
      "gain espéré pour  b 1.5\n",
      "joueur  1\n",
      "gain espéré pour  a 4.0\n",
      "gain espéré pour  b 2.0\n",
      "a a\n",
      "[{'a': 7, 'b': 3}, {'a': 7, 'b': 2}]\n",
      "joueur  0\n",
      "gain espéré pour  a 5.25\n",
      "gain espéré pour  b 1.5\n",
      "joueur  1\n",
      "gain espéré pour  a 4.666666666666667\n",
      "gain espéré pour  b 2.0\n",
      "a a\n",
      "[{'a': 8, 'b': 3}, {'a': 8, 'b': 2}]\n",
      "joueur  0\n",
      "gain espéré pour  a 6.0\n",
      "gain espéré pour  b 1.5\n",
      "joueur  1\n",
      "gain espéré pour  a 5.333333333333333\n",
      "gain espéré pour  b 2.0\n",
      "a a\n",
      "[{'a': 9, 'b': 3}, {'a': 9, 'b': 2}]\n",
      "joueur  0\n",
      "gain espéré pour  a 6.75\n",
      "gain espéré pour  b 1.5\n",
      "joueur  1\n",
      "gain espéré pour  a 6.0\n",
      "gain espéré pour  b 2.0\n",
      "a a\n",
      "[{'a': 10, 'b': 3}, {'a': 10, 'b': 2}]\n",
      "joueur  0\n",
      "gain espéré pour  a 7.5\n",
      "gain espéré pour  b 1.5\n",
      "joueur  1\n",
      "gain espéré pour  a 6.666666666666667\n",
      "gain espéré pour  b 2.0\n",
      "a a\n"
     ]
    }
   ],
   "source": [
    "actions = [('a','b'),('a','b')]\n",
    "matGains ={'a':{'a':(3,2),'b':(0,0)},'b':{'a':(0,0),'b':(2,3)}}\n",
    "prior = [{'a':3,'b':1},{'a':1,'b':2}]\n",
    "nbJoueurs = 2\n",
    "nbObservations = [sum (prior[i].values()) for i in range(nbJoueurs)]\n",
    "nbIterations = 10\n",
    "\n",
    "def fictitious(observations):\n",
    "    actionsChosen =[]\n",
    "    for i in range(nbJoueurs):\n",
    "        expectedGain = {'a':0,'b':0}\n",
    "        bestAction = actions[i][0]\n",
    "        bestGain =0\n",
    "        print ('joueur ',i)\n",
    "        for actionMe in actions[i]: \n",
    "            for actionOther in actions[-i]: \n",
    "                expectedGain[actionMe] += (observations[i][actionMe]/nbObservations[i])*matGains[actionMe][actionOther][i]\n",
    "            print (\"gain espéré pour \",actionMe, expectedGain[actionMe])\n",
    "            if expectedGain[actionMe]>bestGain:\n",
    "                bestAction=actionMe\n",
    "                bestGain=expectedGain[actionMe]\n",
    "        actionsChosen+= bestAction\n",
    "    return actionsChosen\n",
    "\n",
    "observations = prior.copy()\n",
    "for i in range(nbIterations):\n",
    "    print (observations)\n",
    "    nextAction0, nextAction1 = fictitious(observations)\n",
    "    print (nextAction0,nextAction1)\n",
    "    observations[0][nextAction1]+=1\n",
    "    observations[1][nextAction0]+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quelques propriétés des dynamiques fictitious play\n",
    "\n",
    "Supposons à présent que les deux agents utilisent une dynamique de fictitious play. \n",
    "On voit que l'on peut atteindre deux types de situations d'équilibre: \n",
    "* des équilibres pour lesquels les probabilités sont concentrées sur une action unique (équilibre pur)\n",
    "* des équilibres pour lesquels les probabilités sont réparties sur plusieurs actions (équilibre mixte)\n",
    "\n",
    "On sait alors que: \n",
    "\n",
    "* tout équilibre pur atteint par fictitious play doit être un équilibre de Nash\n",
    "* la dynamique n'atteint pas nécessairement de situation d'équilibre (même mixte)\n",
    "* il existe plusieurs catégories de jeux pour lesquels la convergence est garantie, par exemple les jeux à somme nulle. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Dynamique basée sur le regret\n",
    "\n",
    "Appelons: \n",
    "* $score^t$ le score moyen obtenu jusqu'à la période $t$ par l'agent. \n",
    "* $score^t(s)$ le score moyen que l'agent aurait obtenu *s'il avait joué la stratégie* $s$ de manière fixe (toutes choses égales par ailleurs, ie. si les autres agents avaient joué les mêmes stratégies à toutes les étapes)\n",
    "\n",
    "Le **regret** de ne pas avoir joué $s$ est alors, au moment $t$ tout simplement $$R^t(s) = score^t - score^t(s)$$\n",
    "\n",
    "L'agent choisit alors l'action selon une probabilité proportionnelle à son regret. \n",
    "Autrement dit, la règle est simplement de choisir selon la propabilité donnée pour chaque action par: \n",
    "\n",
    "$$ \\frac{R^t(s)}{\\sum_{s' \\in S_i} R^t(s')} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration: calcul de meilleure réponse Rock-Paper-Scissor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on suppose que l'adversaire joue une stratégie mixte. Notre agent va calculer sa meilleure réponse à cette stratégie mixte en simulant l'algorithme de regret matching (self-play). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my strategy is: {'r': 0.3333333333333333, 'p': 0.3333333333333333, 's': 0.3333333333333333}\n",
      "step 0 p s\n",
      "{'r': 2, 'p': 0, 's': 1}\n",
      "my strategy is: {'r': 0.6666666666666666, 'p': 0.0, 's': 0.3333333333333333}\n",
      "step 1 s s\n",
      "{'r': 3, 'p': -1, 's': 1}\n",
      "my strategy is: {'r': 0.75, 'p': 0.0, 's': 0.25}\n",
      "step 2 r r\n",
      "{'r': 3, 'p': 0, 's': 0}\n",
      "my strategy is: {'r': 1.0, 'p': 0.0, 's': 0.0}\n",
      "step 3 r r\n",
      "{'r': 3, 'p': 1, 's': -1}\n",
      "my strategy is: {'r': 0.75, 'p': 0.25, 's': 0.0}\n",
      "step 4 r r\n",
      "{'r': 3, 'p': 2, 's': -2}\n",
      "my strategy is: {'r': 0.6, 'p': 0.4, 's': 0.0}\n",
      "step 5 p r\n",
      "{'r': 2, 'p': 2, 's': -4}\n",
      "my strategy is: {'r': 0.5, 'p': 0.5, 's': 0.0}\n",
      "step 6 r s\n",
      "{'r': 2, 'p': 0, 's': -5}\n",
      "my strategy is: {'r': 1.0, 'p': 0.0, 's': 0.0}\n",
      "step 7 r s\n",
      "{'r': 2, 'p': -2, 's': -6}\n",
      "my strategy is: {'r': 1.0, 'p': 0.0, 's': 0.0}\n",
      "step 8 r s\n",
      "{'r': 2, 'p': -4, 's': -7}\n",
      "my strategy is: {'r': 1.0, 'p': 0.0, 's': 0.0}\n",
      "step 9 r s\n",
      "{'r': 2, 'p': -6, 's': -8}\n",
      "{'r': 1.0, 'p': 0.0, 's': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "actions = [('r','p','s'),('r','p','s')]\n",
    "matGains ={'r':{'r':(0,0),'p':(-1,1),'s':(1,-1)},\\\n",
    "           'p':{'r':(1,-1),'p':(0,0),'s':(-1,1)},\\\n",
    "           's':{'r':(-1,1),'p':(1,-1),'s':(0,0)}}\n",
    "regret = {'r':0,'p':0,'s':0}\n",
    "oppStrategy = {'r':0.4, 'p':0.3, 's':0.3} # on suppose que l'adversaire joue avec cette stratégie mixte\n",
    "           \n",
    "def pickAction(strategy):\n",
    "    r = np.random.uniform(0,1)\n",
    "    cumulStrat = 0\n",
    "    for s in strategy:\n",
    "        cumulStrat+=strategy[s]\n",
    "        if r < cumulStrat:\n",
    "            return s\n",
    "    return\n",
    "           \n",
    "def cumulRegret(myAction,otherAction,regret):\n",
    "    ''' accumule le regret pour chaque action, pour un choix spécifique d'actions\n",
    "    '''\n",
    "    for action in actions[0]:\n",
    "        regret[action]+=matGains[action][otherAction][0]-matGains[myAction][otherAction][0]\n",
    "    return\n",
    "\n",
    "def updateRegretBasedStrategy(regret):\n",
    "    '''calcul de la strategie mixte proportionnelle aux regrets positifs\n",
    "    on prend distribution uniforme si tous les regrets sont à 0\n",
    "    '''\n",
    "    myStrategy = {'r':0,'p':0,'s':0}\n",
    "    for action in myStrategy:\n",
    "        if regret[action]>0: # seulement si le regret est positif\n",
    "            myStrategy[action]=regret[action]\n",
    "    total = sum(myStrategy.values())\n",
    "    if total==0:\n",
    "        return {k:1/len(actions[0]) for k in myStrategy.keys()} \n",
    "    else:\n",
    "        return {k:v/total for k,v in myStrategy.items()}\n",
    "    return\n",
    "    \n",
    "           \n",
    "def simulate(iterations,verbose=False):\n",
    "    for step in range(iterations):\n",
    "        otherAction = pickAction(oppStrategy)\n",
    "        myStrategy = updateRegretBasedStrategy(regret)\n",
    "        myAction = pickAction(myStrategy)\n",
    "        cumulRegret(myAction,otherAction,regret) # calcul le regret induit à cette étape\n",
    "        if verbose:\n",
    "            print(\"my strategy is:\", myStrategy)\n",
    "            print(\"step\",step, myAction,otherAction)\n",
    "            print(regret)\n",
    "    print(myStrategy)\n",
    "    return\n",
    "        \n",
    "           \n",
    "simulate(10,True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si les deux agents jouent en appliquant l'algorithme de regret matching, ils atteignent un équilibre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: equilibrium of RPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Rational Learning (Bayesian learning)\n",
    "\n",
    "On note $S$ l'ensemble des stratégies que l'agent $x$ considère possible pour son adversaire, et $H$ l'ensemble des histoires (séquences de coups) possibles. \n",
    "Il faut noter que contrairement aux approches précédentes, les statégies envisagées peuvent être ici des stratégies non stationnaires (par exemple Tit-for-Tat, etc.).  \n",
    "Ainsi, étant donné une histoire $h$ observée de coups de l'adversaire, la probabilité selon $x$ que l'autre joueur joue $s$ est donnée, selon application de la règle de Bayes, par : \n",
    "$$ P_x(s|h) = \\frac{P_x(h|s)P_x(s)}{\\sum_{s' \\in S} P_x(h | s') P_x(s')} $$\n",
    "\n",
    "Notez que les joueurs doivent avoir des croyances a priori sur la probabilité de l'autre de jouer certaines stratégies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Autres techniques d'apprentissage\n",
    "\n",
    "Considérons le problème suivant: chaque week-end des agents souhaitent se rendre dans un bar pour écouter de la musique. Les agents (supposons qu'ils sont au nombre de 100) ont la préférence suivante: ils souhaitent aller au bar pour écouter la musique, mais en même temps qu'il y ait trop de monde, sinon ils ne peuvent plus écouter correctement de la musique. \n",
    "\n",
    "Plus précisément: \n",
    "\n",
    "* si moins de 60 % de la population se rend au bar, l'agent préfère aller au bar\n",
    "* si plus de 60% de la population se rend au bar, l'agent aurait préféré rester chez lui. \n",
    "\n",
    "Ls agents se rendent le premier week-end au bar, ils observent la situation, puis révise leur stratégie pour le deuxième week-end, etc.\n",
    "\n",
    "Il s'agit d'un problème connu sous le nom de *El-Farol problem*, un jeu de minorité. \n",
    "\n",
    "On s'interroge sur la règle de décision que peuvent utiliser les agens dans cette seituation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Que se passe-t-il si tous les agents utilisent la même méthode déterministe pour décider s'ils vont au bar ou pas?\n",
    "\n",
    "> Que se passe-t-il si tous les agents utilisent pile ou face pour décider s'ils vont au bar ou pas?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arthur propose de faire reposer la décision des agents sur un **ensemble de prédicteurs**. \n",
    "\n",
    "* prédicteurs cycliques: même nombre de personnes qu'il y a *k* semaines\n",
    "* 100 - le nombre de personnes de la semaine dernière\n",
    "* prédicteur fixe: 67 personnes\n",
    "* moyenne glissante sur les 4 dernières semaines\n",
    "* la tendance sur les 8 dernières semaines évaluée avec une régression linéaire et la méthode des moindre carrés\n",
    "\n",
    "Chaque prédicteur se voit ensuite associé un score, selon la qualité de sa prédiction (+1 si la prédiction trop-de-monde / pas-trop-de-monde concorde avec l'observation). \n",
    "\n",
    "Une bibliothèque de prédicteurs est créée, puis chaque agent reçoit un ensemble de prédicteurs qu'il aura à sa disposition. A chaque tour, les agents utiliseront leur prédicteur le plus performant jusqu'alors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> Version du 2018/02/28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
